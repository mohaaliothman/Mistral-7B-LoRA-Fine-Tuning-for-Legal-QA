{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Setup</h1>"
      ],
      "metadata": {
        "id": "OtUwKkqG_EmX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_VVtGBf-O1q"
      },
      "outputs": [],
      "source": [
        "!pip install -U trl peft bitsandbytes transformers datasets accelerate evaluate bert-score rouge_score --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Imports & Login</h1>"
      ],
      "metadata": {
        "id": "x3UrKWzb_S7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=os.getenv(\"HF_TOKEN\"))  # Replace with your token\n"
      ],
      "metadata": {
        "id": "relzSZM-7l5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n"
      ],
      "metadata": {
        "id": "GlobqHlB-VxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Model & Dataset</h1>\n"
      ],
      "metadata": {
        "id": "udbwNrQG_bMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"haistudy/en_law_qa\", split=\"train\")\n",
        "instruction = \"You are a Law Assistant. Please answer the following question.\"\n",
        "\n",
        "def format_chat(row):\n",
        "    row[\"text\"] = f\"[INST] {instruction} {row['Question']} [/INST] {row['Answer']}\"\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(format_chat)\n",
        "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "eJAmUuFv-cxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>LoRA Setup</h1>"
      ],
      "metadata": {
        "id": "TTpmCum3_iJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "\n",
        "def find_all_linear_names(model):\n",
        "    cls = bnb.nn.Linear4bit\n",
        "    names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names.add(name.split('.')[-1])\n",
        "    return list(names - {\"lm_head\"})\n",
        "\n",
        "target_modules = find_all_linear_names(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=target_modules\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "I7VvoGpl-9Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Training</h1>"
      ],
      "metadata": {
        "id": "FFhqTKop_mtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "tokenizer.truncation_side = \"left\"\n",
        "tokenizer.model_max_length = 1024\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    output_dir=\"mistral-law-finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    fp16=True,\n",
        "    group_by_length=True,\n",
        "    report_to=\"none\",\n",
        "    max_length=512,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    peft_config=lora_config,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ea74fDqJ_A0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Ej5GPRHD77eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Evaluation</h1>"
      ],
      "metadata": {
        "id": "JG7jHTbV_rn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "from bert_score import score as bert_score\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "def simple_f1(pred, label):\n",
        "    pred_tokens = pred.split()\n",
        "    label_tokens = label.split()\n",
        "    common = set(pred_tokens) & set(label_tokens)\n",
        "    if not common: return 0.0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(label_tokens)\n",
        "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0.0\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, max_new_tokens=60, batch_size=8):\n",
        "    model.eval()\n",
        "    rouge = load(\"rouge\")\n",
        "    results, f1s, preds, labels = [], [], [], []\n",
        "\n",
        "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "        batch = dataset[i:i+batch_size]\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True, truncation=True,\n",
        "                           max_length=tokenizer.model_max_length).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids=inputs[\"input_ids\"],\n",
        "                                     attention_mask=inputs[\"attention_mask\"],\n",
        "                                     max_new_tokens=max_new_tokens, do_sample=False)\n",
        "\n",
        "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        decoded_labels = batch[\"Answer\"]\n",
        "\n",
        "        for pred, label in zip(decoded_preds, decoded_labels):\n",
        "            pred, label = pred.strip(), label.strip()\n",
        "            preds.append(pred)\n",
        "            labels.append(label)\n",
        "            f1s.append(simple_f1(pred, label))\n",
        "            results.append({\"prediction\": pred, \"reference\": label, \"f1\": f1s[-1]})\n",
        "\n",
        "        del inputs, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    rougeL = rouge.compute(predictions=preds, references=labels)[\"rougeL\"]\n",
        "    _, _, bert_F1 = bert_score(preds, labels, lang=\"en\", device=\"cpu\")\n",
        "\n",
        "    print(f\"\\nAverage F1 Score:  {sum(f1s)/len(f1s):.4f}\")\n",
        "    print(f\"ROUGE-L Score:     {rougeL:.4f}\")\n",
        "    print(f\"BERTScore (F1):    {bert_F1.mean().item():.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"f1\": sum(f1s) / len(f1s),\n",
        "        \"rougeL\": rougeL,\n",
        "        \"bertscore_f1\": bert_F1.mean().item(),\n",
        "        \"results\": results\n",
        "    }\n",
        "\n",
        "# Run evaluation\n",
        "metrics = evaluate_model(model, tokenizer, eval_dataset)\n",
        "\n",
        "'''Average F1 Score:  0.6408\n",
        "ROUGE-L Score:     0.6966\n",
        "BERTScore (F1):    0.9337'''\n"
      ],
      "metadata": {
        "id": "9cBlH_nB_DAH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}